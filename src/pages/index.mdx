---
layout: ../layouts/Layout.astro
title: Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning
description: AKR project page
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";
import Carousel from "../components/Carousel.tsx";

import { ImageComparison } from "../components/ImageComparison.tsx";

import overview from "../assets/fig1-overview.png"
import { embodimentVideos } from '../data/carouselVideos.ts';
import { skillVideos } from '../data/carouselVideos.ts';
import { sceneVideos } from '../data/carouselVideos.ts';

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Ziyuan Jiao",
      url: "https://scholar.google.com/citations?user=YfWhJZoAAAAJ&hl=en",
      institution: "BIGAI",
    },
    {
      name: "Yida Niu",
      url: "https://github.com/fi6",
      institution: "BIGAI, Peking University",
    },
    {
      name: "Zeyu Zhang",
      url: "https://zeyuzhang.com",
      institution: "BIGAI",
    },
    {
      name: "Yangyang Wu",
      url: "https://github.com/OliverWu1996",
      institution: "Peking University",
    },
    {
      name: "Yao Su",
      url: "https://yaosu.info",
      institution: "BIGAI",
    },
    {
      name: "Yixin Zhu",
      url: "https://yzhu.io",
      institution: "Peking University",
    },
    {
      name: "Hangxin Liu",
      url: "https://liuhx111.github.io",
      institution: "BIGAI",
    },
    {
      name: "Song-Chun Zhu",
      url: "https://zhusongchun.net/",
      institution: "BIGAI, Peking University",
    },
  ]}
  conference="Transactions on Robotics"
  notes={[
    {
      text: "If you have any question, please contact jiaoziyuan at bigai dot ai.",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "https://ieeexplore.ieee.org/document/11146897",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/zyjiao4728/AKR-Planning/tree/release",
      icon: "ri:github-line",
    },
    {
      name: "PDDL",
      url: "https://github.com/zyjiao4728/AKR-PDDL",
      icon: "ri:github-line"
    },
    {
      name: "arXiv",
      url: "https://arxiv.org/abs/2508.18627",
      icon: "academicons:arxiv",
    },
  ]}
  />


<Figure>
  <Image slot="figure" source={overview} altText="An exemple to illustrate the advancement of proposed SMMP framework with A-Space compared to traditional planning." />
  <span slot="caption">An exemple to illustrate the advancement of proposed SMMP framework with A-Space compared to traditional planning.</span>
</Figure>

<HighlightedSection>

## Abstract

We present a Sequential Mobile Manipulation Planning (SMMP) framework that can solve long-horizon multi-step mobile manipulation tasks with coordinated whole-body motion, even when interacting with articulated objects. By abstracting environmental structures as kinematic models and integrating them with the robot's kinematics, we construct an Augmented Configuration Apace (A-Space) that unifies the previously separate task constraints for navigation and manipulation, while accounting for the joint reachability of the robot base, arm, and manipulated objects. This integration facilitates efficient planning within a tri-level framework: a task planner generates symbolic action sequences to model the evolution of A-Space, an optimization-based motion planner computes continuous trajectories within A-Space to achieve desired configurations for both the robot and scene elements, and an intermediate plan refinement stage selects action goals that ensure long-horizon feasibility. Our simulation studies first confirm that planning in A-Space achieves an 84.6\% higher task success rate compared to baseline methods. Validation on real robotic systems demonstrates fluid mobile manipulation involving (i) seven types of rigid and articulated objects across 17 distinct contexts, and (ii) long-horizon tasks of up to 14 sequential steps. Our results highlight the significance of modeling scene kinematics into planning entities, rather than encoding task-specific constraints, offering a scalable and generalizable approach to complex robotic manipulation.

</HighlightedSection>


## Videos
### Whole-body mobile manipulation of different articulate objects.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="ol5vkf9jb8M" />
  </Figure>
</div>

### Sequential mobile manipulation tasks in a household environment.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="wrC0FHQCAUY" />
  </Figure>
</div>

### Cross-embodiment deployment and tool-use tasks.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="Oh4533eYdJE" />
  </Figure>
</div>

### Long-horizon SMMP with combined articulate object manipulation and tool-use.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="pUeKHHS2d3k" />
  </Figure>
</div>

### A sequential mobile manipulation task planned and executed reactively in real-time.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="lU2401F4JMo"/>      
  </Figure>
</div>

## Extended Simulation Results

To demonstrate the generality and applicability of our method across diverse object structures and realistic environments, we conducted an extensive evaluation in cluttered household scenes adopted from iThor. Each scene included articulated objects from the PartNet-Mobility dataset, replacing existing objects of the same category to preserve contextual realism. We used AO-Grasp to generate candidate grasps per object and tested on three new mobile manipulators. To manage the scale and complexity, we ported our AKR-based motion planner to the GPU-accelerated Curobo platform and developed a fully automated environment setup pipeline. A gallery of results is presented below.


### Diverse Scenes
<Carousel 
  videos={sceneVideos}
  client:load
/>

### Diverse Embodiments and Manipulation Skills

<Carousel 
  videos={[...embodimentVideos, ...skillVideos]}
  client:load
/>



{/* ## BibTeX citation

```bibtex
@misc{roman2024academic,
  author = "{Roman Hauksson}",
  title = "Academic Project Page Template",
  year = "2024",
  howpublished = "\url{https://research-template.roman.technology}",
}
```
*/}

## External links

The proposed AKR-based mobile manipulation planner supports data generation pipelines such as [AutoMoMa](https://automoma.pages.dev/) and [m3bench](https://zeyuzhang.com/papers/m3bench/), facilitating the development of learning-based methods like [m2diffuser](https://m2diffuser.github.io/) for mobile manipulation skills.

