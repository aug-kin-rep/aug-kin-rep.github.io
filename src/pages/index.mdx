---
layout: ../layouts/Layout.astro
title: Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"
import overview from "../assets/fig1-overview.png"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Ziyuan Jiao",
      url: "https://scholar.google.com/citations?user=YfWhJZoAAAAJ&hl=en",
      institution: "BIGAI",
    },
    {
      name: "Yida Niu",
      url: "https://github.com/fi6",
      institution: "Peking University, BIGAI",
    },
    {
      name: "Zeyu Zhang",
      url: "https://zeyuzhang.com",
      institution: "BIGAI",
    },
    {
      name: "Yangyang Wu",
      url: "https://github.com/OliverWu1996",
      institution: "Peking University",
    },
    {
      name: "Yao Su",
      url: "https://yaosu.info",
      institution: "BIGAI",
    },
    {
      name: "Yixin Zhu",
      url: "https://yzhu.io",
      institution: "Peking University",
    },
    {
      name: "Hangxin Liu",
      url: "https://liuhx111.github.io",
      institution: "BIGAI",
    },
    {
      name: "Song-Chun Zhu",
      url: "https://zhusongchun.net/",
      institution: "BIGAI",
    },
  ]}
  conference="In Submission"
  notes={[
    {
      text: "If you have any question, please contact jiaoziyuan at bigai dot ai.",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code (Coming Soon)",
      url: "",
      icon: "ri:github-line",
    },
    {
      name: "PDDL",
      url: "https://github.com/",
      icon: "ri:github-line"
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
  ]}
  />

<Figure>
  <Image slot="figure" source={overview} altText="An exemple to illustrate the advancement of proposed SMMP framework with A-Space compared to traditional planning." />
  <span slot="caption">An exemple to illustrate the advancement of proposed SMMP framework with A-Space compared to traditional planning.</span>
</Figure>

<HighlightedSection>

## Abstract

We present a Sequential Mobile Manipulation Planning (SMMP) framework that can solve long-horizon multi-step mobile manipulation tasks with coordinated whole-body motion, even when interacting with articulated objects. By abstracting environmental structures as kinematic models and integrating them with the robot's kinematics, we construct an Augmented Configuration Apace (A-Space) that unifies the previously separate task constraints for navigation and manipulation, while accounting for the joint reachability of the robot base, arm, and manipulated objects. This integration facilitates efficient planning within a tri-level framework: a task planner generates symbolic action sequences to model the evolution of A-Space, an optimization-based motion planner computes continuous trajectories within A-Space to achieve desired configurations for both the robot and scene elements, and an intermediate plan refinement stage selects action goals that ensure long-horizon feasibility. Our simulation studies first confirm that planning in A-Space achieves an 84.6\% higher task success rate compared to baseline methods. Validation on real robotic systems demonstrates fluid mobile manipulation involving (i) seven types of rigid and articulated objects across 17 distinct contexts, and (ii) long-horizon tasks of up to 14 sequential steps. Our results highlight the significance of modeling scene kinematics into planning entities, rather than encoding task-specific constraints, offering a scalable and generalizable approach to complex robotic manipulation.

</HighlightedSection>


## Videos
### Whole-body mobile manipulation of different articulate objects.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="ol5vkf9jb8M" />
  </Figure>
</div>

### Sequential mobile manipulation tasks in a household environment.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="wrC0FHQCAUY" />
  </Figure>
</div>

### Whole-body mobile manipulation for tool-use tasks.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="Oh4533eYdJE" />
  </Figure>
</div>

### Mobile manipulation planning with combined articulate object manipulation and tool-use.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="pUeKHHS2d3k" />
  </Figure>
</div>

### Whole-body sequential mobile manipulation planned and executed in real-time.
<div style="width: 50%;">
  <Figure>
      <YouTubeVideo slot="figure" videoId="lU2401F4JMo"/>      
  </Figure>
</div>

## Extended Simulation Results

To demonstrate the generality and applicability of our method across diverse object structures and realistic environments, we conducted an extensive evaluation in 30 cluttered household scenes from iThor. Each scene included five articulated objects from the PartNet-Mobility dataset, replacing existing objects of the same category to preserve contextual realism. We used AO-Grasp to generate 20 candidate grasps per object and tested three new mobile manipulators. To manage the scale and complexity, we ported our AKR-based motion planner to the GPU-accelerated Curobo platform and developed a fully automated environment setup pipeline. A gallery of results is presented below.


{/* ## BibTeX citation

```bibtex
@misc{roman2024academic,
  author = "{Roman Hauksson}",
  title = "Academic Project Page Template",
  year = "2024",
  howpublished = "\url{https://research-template.roman.technology}",
}
``` */}